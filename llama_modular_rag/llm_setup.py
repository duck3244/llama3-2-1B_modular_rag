import torch
from typing import Any
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from config import LLAMA_MODEL_PATH, DEVICE, TEMPERATURE, MAX_NEW_TOKENS


def setup_llama_model() -> HuggingFacePipeline:
    """CPUì— ìµœì í™”ëœ Llama ëª¨ë¸ ì„¤ì •"""
    print("ğŸ”§ Llama ëª¨ë¸ì„ CPU ëª¨ë“œë¡œ ë¡œë“œ ì¤‘...")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_PATH)

    # ëª¨ë¸ ë¡œë“œ ìµœì í™” ì˜µì…˜ (CPU ê°•ì œ)
    model = AutoModelForCausalLM.from_pretrained(
        LLAMA_MODEL_PATH,
        torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
        low_cpu_mem_usage=True,
        device_map={"": "cpu"},  # ëª…ì‹œì ìœ¼ë¡œ CPU ì§€ì •
        trust_remote_code=True
    )

    # ëª¨ë¸ì„ CPUë¡œ ëª…ì‹œì  ì´ë™ ë° ìµœì í™”
    model = model.to("cpu")
    model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
    
    # CUDA í…ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
    if next(model.parameters()).is_cuda:
        print("âš ï¸  ê²½ê³ : ëª¨ë¸ì´ GPUì— ìˆìŠµë‹ˆë‹¤. CPUë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        model = model.cpu()
    
    print(f"âœ… ëª¨ë¸ì´ CPUì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")

    # íŒŒì´í”„ë¼ì¸ ìƒì„± (CPU ê°•ì œ)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=MAX_NEW_TOKENS,  # í† í° ìƒì„± ì œí•œ
        temperature=TEMPERATURE,
        repetition_penalty=1.1,
        batch_size=1,  # CPUì—ì„œëŠ” ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        device=-1  # CPU ì‚¬ìš© (-1ì€ CPUë¥¼ ì˜ë¯¸)
    )

    # LangChain ëª¨ë¸ ìƒì„±
    llm: HuggingFacePipeline = HuggingFacePipeline(pipeline=pipe)
    return llm
